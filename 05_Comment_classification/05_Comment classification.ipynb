{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация комментариев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Описание проекта\n",
    "\n",
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. \n",
    "\n",
    "Необходимо построить и обучить модель классифицировать комментарии на позитивные и негативные для дальнейшей обработки сервисной службой.\n",
    "\n",
    "Эта модель должна иметь значение метрики качества F1 не меньше 0.75.\n",
    "\n",
    "\n",
    "##### План по выполнению проекта\n",
    "- Загрузка и подготовка данных\n",
    "- Обучение несколько моделей\n",
    "- Выводы\n",
    "\n",
    "##### Описание данных\n",
    "Данные находятся в файле toxic_comments.csv.\n",
    "\n",
    "Данные содержат столбцы:\n",
    "- text - текст комментария\n",
    "- toxic - целевой признак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mikalai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mikalai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mikalai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import notebook\n",
    "from pymystem3 import Mystem\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и выведем информацию о данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "#data = pd.read_csv('/Users/Mikalai/Documents/Data Science/Проект Машинное обучение для текстов/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные загружены.\n",
    "\n",
    "В таблице 159 292 строк и 3 столбца:\n",
    "- Unnamed: 0 - ненужный столбец, который надо удалить\n",
    "- text - текст комментария\n",
    "- toxic - целевой признак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужный столбец Unnamed: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "data.shape                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.101612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.302139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic\n",
       "count  159292.000000\n",
       "mean        0.101612\n",
       "std         0.302139\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим данные на пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим баланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов класса 0: 143106 (89.84%)\n",
      "Процент объектов класса 1: 16186 (10.16%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Процент объектов класса 0: {data['toxic'].value_counts()[0]} ({(data['toxic'].value_counts()[0]/data.shape[0])*100:.2f}%)\")\n",
    "print(f\"Процент объектов класса 1: {data['toxic'].value_counts()[1]} ({(data['toxic'].value_counts()[1]/data.shape[0])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы:\n",
    "- отсутствуют пропуски в данных\n",
    "- классы несбалансированы. Это надо учесть при обучении моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим переменную корпуса текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию лемматизации и функцию очистки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    clear_text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    clear_text = clear_text.lower().split()\n",
    "    return ' '.join(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает функция на тексте первой строки обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edits made under my username hardcore metallica fan were reverted they weren t vandalism just closure on some gas after i voted at new york doll fac and please don t remove the template from the talk page since i m retired now'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(clear_text(data.loc[0, 'text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем очистку и лемматизацию текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text'] = data['text'].apply(clear_text)\n",
    "data['lemm_text'] = data['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим в данных стобцец lemm_text с очищеным и лематизированным текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>congratulation from me a well use the tool wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>sorry if the word nonsense wa offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                          lemm_text\n",
       "0      0  explanation why the edits made under my userna...\n",
       "1      0  d aww he match this background colour i m seem...\n",
       "2      0  hey man i m really not trying to edit war it s...\n",
       "3      0  more i can t make any real suggestion on impro...\n",
       "4      0  you sir are my hero any chance you remember wh...\n",
       "5      0  congratulation from me a well use the tool wel...\n",
       "6      1       cocksucker before you piss around on my work\n",
       "7      0  your vandalism to the matt shirvington article...\n",
       "8      0  sorry if the word nonsense wa offensive to you...\n",
       "9      0  alignment on this subject and which are contra..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочную и тестовую выборки в соотношении 4:1. \n",
    "\n",
    "Чтобы выборки были более сбалансированы стратифицируем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    data.drop('toxic', axis=1),\n",
    "    data['toxic'],\n",
    "    test_size=0.25,\n",
    "    random_state=12345,\n",
    "    stratify=data['toxic'] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим размеры выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки: 119469\n",
      "Размер тестовой выборки: 39823\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочной выборки: {len(train_features)}\")\n",
    "print(f\"Размер тестовой выборки: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим корпус из очищеных и лематизированных тексов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = train_features['lemm_text'].astype('U')\n",
    "corpus_test = test_features['lemm_text'].astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов учитывает частоту употребления слов. Оценка важности слова определяется величиной TF-IDF (от англ. term frequency, «частота терма, или слова»; inverse document frequency, «обратная частота документа, или текста»).\n",
    "\n",
    "То есть TF отвечает за количество упоминаний слова в отдельном тексте, а IDF отражает частоту его употребления во всём корпусе.\n",
    "\n",
    "Воспользуемся счётчиком величин TF-IDF  - TfidfVectorizer. Чтобы почистить мешок слов, добавим в него стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (119469, 133602)\n",
      "Размер матрицы: (39823, 133602)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stoplist) \n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train) \n",
    "tf_idf_test = count_tf_idf.transform(corpus_test) \n",
    "\n",
    "print(\"Размер матрицы:\", tf_idf_train.shape)\n",
    "print(\"Размер матрицы:\", tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы\n",
    "\n",
    "В ходе подготовки данных выполнены:\n",
    "1. Выполнили загрузку данных\n",
    "\n",
    "В таблице 159 292 строк и 3 столбца:\n",
    "- Unnamed: 0 - ненужный столбец, который удалили\n",
    "- text - текст комментария\n",
    "- toxic - целевой признак\n",
    "\n",
    "В данных отсутствуют пропуски \n",
    "\n",
    "Классы целевого признака несбалансированы. Это надо учесть при обучении моделей\n",
    "\n",
    "2. Выполнена предобработка текста\n",
    "- проведена очистка и лемматизация текста\n",
    "- данные разделены на тренировочную и тестовую выборки в соотношении 4:1\n",
    "- к текстам применена векторизация TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как задача сводится к задаче классификации, то выберем следующие модели машинного обучения:\n",
    "- логистическую регрессию LogisticRegression\n",
    "- решающее дерево RandomForestClassifier\n",
    "- градиентного бустинга LGBMClassifier\n",
    "\n",
    "Так как классы целевого признака несбалансированы, в моделях используем параметр class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Обучение модели LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель LinearRegression с базовым набором гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_lr = LogisticRegression(class_weight = 'balanced')\n",
    "model_lr.fit(tf_idf_train, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика F1 на модели LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1 на модели LogisticRegression: 0.745'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(model_lr, tf_idf_train, train_target, cv=4, scoring='f1')\n",
    "f\"F1 на модели LogisticRegression: {round(score.mean(), 3)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Обучение модели DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем гиперпараметры для модели DecisionTreeClassifier и обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Лучшие параметры DecisionTreeClassifier: DecisionTreeClassifier(class_weight='balanced', max_depth=48,\n",
      "                       random_state=12345)\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = DecisionTreeClassifier(class_weight = 'balanced', random_state = 12345)\n",
    "\n",
    "parameter_grid = {\n",
    "    'max_depth':[x for x in range(30,50,2)],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_searcher_tree = RandomizedSearchCV(clf, parameter_grid, scoring='f1', cv=3, verbose=2, n_jobs = -1)\n",
    "\n",
    "grid_searcher_tree.fit(tf_idf_train, train_target)\n",
    "\n",
    "print('Лучшие параметры DecisionTreeClassifier:', grid_searcher_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика F1 на модели DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1 на модели DecisionTreeClassifier: 0.623'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"F1 на модели DecisionTreeClassifier: {round(grid_searcher_tree.best_score_, 3)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Обучение модели LGBMClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем гиперпараметры для модели LGBMClassifier и обучим модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Лучшие параметры LGBMClassifier: LGBMClassifier(class_weight='balanced', max_depth=17, n_estimators=500,\n",
      "               random_state=12345)\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LGBMClassifier(class_weight = 'balanced', random_state = 12345) \n",
    "     \n",
    "parameter_grid = {\n",
    "            'max_depth': [15, 17],\n",
    "            'n_estimators': [500]\n",
    "}\n",
    "               \n",
    "grid_searcher_LGMB = RandomizedSearchCV(clf, parameter_grid, scoring='f1', verbose=2, cv=3, n_jobs=-1)\n",
    " \n",
    "grid_searcher_LGMB.fit(tf_idf_train, train_target)                    \n",
    "                    \n",
    "print('Лучшие параметры LGBMClassifier:', grid_searcher_LGMB.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F1 на модели XGBClassifier: 0.761'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"F1 на модели XGBClassifier: {round(grid_searcher_LGMB.best_score_, 3)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем все модели на качество предсказания\n",
    "\n",
    "Модель логистическая регрессия LogisticRegression:\n",
    "- F1 - 0.745\n",
    "\n",
    "Модель решающее дерево DecisionTreeClassifier:\n",
    "- F1 - 0.623\n",
    "\n",
    "Модель градиентного бустинга LGBMClassifier:\n",
    "- F1 - 0.761\n",
    "\n",
    "Лучшее значение метрики F1 у модели градиентного бустинга LGBMClassifier 0.761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним тестирование на тестовой выборке модели LGBMClassifier, которая показала лучшую метрику F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на модели LGBMClassifier на тестовой выборке: 0.7695895522388059\n"
     ]
    }
   ],
   "source": [
    "predicted_LGBM = grid_searcher_LGMB.predict(tf_idf_test)\n",
    "score = f1_score(test_target, predicted_LGBM)\n",
    "print('F1 на модели LGBMClassifier на тестовой выборке:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение метрики F1 лучшей модели LGBMClassifier на тестовой выборке 0.769, что больше заданного в условии порога 0.75 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем проверку лучшей модели на адекватность, сравнив качество её предсказаний и качеством предсказания константной модели DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(strategy='stratified')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='stratified')\n",
    "dummy.fit(tf_idf_train, train_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на модели DummyClassifier на тестовой выборке: 0.10697443534375775\n"
     ]
    }
   ],
   "source": [
    "predicted_dummy = dummy.predict(tf_idf_test)\n",
    "score = f1_score(test_target, predicted_dummy)\n",
    "print('F1 на модели DummyClassifier на тестовой выборке:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество предсказания метрика F1 константной модели DummyRegressor на тестовой выборке 0.107, что значительно хуже качества предсказания лучшей модели LGBMClassifier на тренировочной выборке. Это свидетельствует об адекватности модели LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### В ходе работы над проектом было выполнено:\n",
    "\n",
    "1. Загрузка данных\n",
    "\n",
    "В таблице 159 292 строк и 3 столбца:\n",
    "- Unnamed: 0 - ненужный столбец, который удалили\n",
    "- text - текст комментария\n",
    "- toxic - целевой признак\n",
    "\n",
    "В данных отсутствуют пропуски \n",
    "\n",
    "Классы целевого признака несбалансированы. Это надо учесть при обучении моделей\n",
    "\n",
    "2. Предобработка текста\n",
    "- проведена очистка и лемматизация текста\n",
    "- данные разделены на тренировочную и тестовую выборки в соотношении 4:1\n",
    "- к текстам применена векторизация TF-IDF\n",
    "\n",
    "3. Обучение следующих моделей:\n",
    "- логистическую регрессию LogisticRegression\n",
    "- решающее дерево RandomForestClassifier\n",
    "- градиентного бустинга LGBMClassifier\n",
    "\n",
    "Так как классы целевого признака несбалансированы в моделях использовали параметр class_weight = 'balanced'\n",
    "\n",
    "4. Оценка качества предсказания на обученных моделях по метрике F1 \n",
    "\n",
    "Лучшей моделью оказалась модель градиентного бустинга LGBMClassifier со значением метрики F1 0.769.\n",
    "\n",
    "5. Тестирование лучшей модели градиентного бустинга LGBMClassifier. Значение финальной метрики лучшей модели LGBMClassifier на тестовой выборке 0.769, что больше заданного в условии порога 0.75\n",
    "\n",
    "6. Проверка лучшей модели на адекватность, сравнив качество её предсказаний и качество предсказания константной модели DummyClassifier\n",
    "\n",
    "##### Таким образом, модель градиентного бустинга LGBMClassifier рекомендуется использовать как инструмент, который будет искать токсичные комментарии"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1628,
    "start_time": "2022-08-16T10:10:46.571Z"
   },
   {
    "duration": 127,
    "start_time": "2022-08-16T10:10:48.201Z"
   },
   {
    "duration": 457,
    "start_time": "2022-08-16T10:11:22.449Z"
   },
   {
    "duration": 17,
    "start_time": "2022-08-16T10:11:30.912Z"
   },
   {
    "duration": 17,
    "start_time": "2022-08-16T10:11:37.929Z"
   },
   {
    "duration": 18,
    "start_time": "2022-08-16T10:11:41.285Z"
   },
   {
    "duration": 52,
    "start_time": "2022-08-16T10:12:05.107Z"
   },
   {
    "duration": 2063,
    "start_time": "2022-09-16T20:35:26.006Z"
   },
   {
    "duration": 120,
    "start_time": "2022-09-16T20:35:28.071Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.194Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.196Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.198Z"
   },
   {
    "duration": 1,
    "start_time": "2022-09-16T20:35:28.198Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.200Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.201Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.202Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.221Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.222Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.223Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.224Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.225Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.226Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.227Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.228Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.230Z"
   },
   {
    "duration": 0,
    "start_time": "2022-09-16T20:35:28.230Z"
   },
   {
    "duration": 1709,
    "start_time": "2022-09-17T17:59:04.244Z"
   },
   {
    "duration": 91,
    "start_time": "2022-09-17T17:59:05.955Z"
   },
   {
    "duration": 6,
    "start_time": "2022-09-17T17:59:06.048Z"
   },
   {
    "duration": 14,
    "start_time": "2022-09-17T17:59:06.055Z"
   },
   {
    "duration": 14,
    "start_time": "2022-09-17T17:59:06.071Z"
   },
   {
    "duration": 7,
    "start_time": "2022-09-17T17:59:06.086Z"
   },
   {
    "duration": 10,
    "start_time": "2022-09-17T17:59:06.095Z"
   },
   {
    "duration": 14,
    "start_time": "2022-09-17T17:59:06.107Z"
   },
   {
    "duration": 36,
    "start_time": "2022-09-17T17:59:06.126Z"
   },
   {
    "duration": 4,
    "start_time": "2022-09-17T17:59:06.164Z"
   },
   {
    "duration": 9,
    "start_time": "2022-09-17T17:59:06.169Z"
   },
   {
    "duration": 16,
    "start_time": "2022-09-17T17:59:06.180Z"
   },
   {
    "duration": 28,
    "start_time": "2022-09-17T17:59:06.200Z"
   },
   {
    "duration": 98,
    "start_time": "2022-09-17T17:59:06.237Z"
   },
   {
    "duration": 100,
    "start_time": "2022-09-17T17:59:06.339Z"
   },
   {
    "duration": 24,
    "start_time": "2022-09-17T17:59:06.441Z"
   },
   {
    "duration": 16,
    "start_time": "2022-09-17T17:59:06.525Z"
   },
   {
    "duration": 93,
    "start_time": "2022-09-17T17:59:06.544Z"
   },
   {
    "duration": 93,
    "start_time": "2022-09-17T17:59:06.639Z"
   },
   {
    "duration": 1831,
    "start_time": "2022-10-14T16:47:30.807Z"
   },
   {
    "duration": 183,
    "start_time": "2022-10-14T16:47:32.640Z"
   },
   {
    "duration": 12,
    "start_time": "2022-10-14T16:47:32.825Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-14T16:47:32.839Z"
   },
   {
    "duration": 17,
    "start_time": "2022-10-14T16:47:32.846Z"
   },
   {
    "duration": 25,
    "start_time": "2022-10-14T16:47:32.865Z"
   },
   {
    "duration": 17,
    "start_time": "2022-10-14T16:47:32.894Z"
   },
   {
    "duration": 8,
    "start_time": "2022-10-14T16:47:32.917Z"
   },
   {
    "duration": 12,
    "start_time": "2022-10-14T16:47:32.926Z"
   },
   {
    "duration": 20,
    "start_time": "2022-10-14T16:47:32.940Z"
   },
   {
    "duration": 13,
    "start_time": "2022-10-14T16:47:32.962Z"
   },
   {
    "duration": 456,
    "start_time": "2022-10-14T16:47:32.977Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-14T16:47:33.435Z"
   },
   {
    "duration": 1009,
    "start_time": "2022-10-14T16:47:33.443Z"
   },
   {
    "duration": 893,
    "start_time": "2022-10-14T16:47:34.454Z"
   },
   {
    "duration": 4667,
    "start_time": "2022-10-14T16:47:35.350Z"
   },
   {
    "duration": 292,
    "start_time": "2022-10-14T16:47:40.019Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.314Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.314Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.317Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.317Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.318Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.320Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.321Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.322Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.322Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.323Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.325Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.326Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.327Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.328Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.329Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.331Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.331Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.333Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.334Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.334Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.336Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.337Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.338Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.341Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.385Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:47:40.385Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-14T16:47:50.319Z"
   },
   {
    "duration": 5,
    "start_time": "2022-10-14T16:47:55.432Z"
   },
   {
    "duration": 3673,
    "start_time": "2022-10-14T16:48:34.220Z"
   },
   {
    "duration": 2219,
    "start_time": "2022-10-14T16:53:40.831Z"
   },
   {
    "duration": 54,
    "start_time": "2022-10-14T16:53:43.053Z"
   },
   {
    "duration": 13,
    "start_time": "2022-10-14T16:53:43.109Z"
   },
   {
    "duration": 11,
    "start_time": "2022-10-14T16:53:43.124Z"
   },
   {
    "duration": 15,
    "start_time": "2022-10-14T16:53:43.136Z"
   },
   {
    "duration": 22,
    "start_time": "2022-10-14T16:53:43.153Z"
   },
   {
    "duration": 12,
    "start_time": "2022-10-14T16:53:43.186Z"
   },
   {
    "duration": 36,
    "start_time": "2022-10-14T16:53:43.201Z"
   },
   {
    "duration": 8,
    "start_time": "2022-10-14T16:53:43.239Z"
   },
   {
    "duration": 22,
    "start_time": "2022-10-14T16:53:43.249Z"
   },
   {
    "duration": 14,
    "start_time": "2022-10-14T16:53:43.273Z"
   },
   {
    "duration": 508,
    "start_time": "2022-10-14T16:53:43.298Z"
   },
   {
    "duration": 10,
    "start_time": "2022-10-14T16:53:43.808Z"
   },
   {
    "duration": 875,
    "start_time": "2022-10-14T16:53:43.820Z"
   },
   {
    "duration": 792,
    "start_time": "2022-10-14T16:53:44.697Z"
   },
   {
    "duration": 3098,
    "start_time": "2022-10-14T16:53:45.491Z"
   },
   {
    "duration": 10,
    "start_time": "2022-10-14T16:53:48.597Z"
   },
   {
    "duration": 111,
    "start_time": "2022-10-14T16:53:48.613Z"
   },
   {
    "duration": 16,
    "start_time": "2022-10-14T16:53:48.726Z"
   },
   {
    "duration": 14,
    "start_time": "2022-10-14T16:53:48.744Z"
   },
   {
    "duration": 7,
    "start_time": "2022-10-14T16:53:48.759Z"
   },
   {
    "duration": 22,
    "start_time": "2022-10-14T16:53:48.768Z"
   },
   {
    "duration": 14,
    "start_time": "2022-10-14T16:53:48.797Z"
   },
   {
    "duration": 88,
    "start_time": "2022-10-14T16:53:48.813Z"
   },
   {
    "duration": 293,
    "start_time": "2022-10-14T16:53:48.903Z"
   },
   {
    "duration": 996,
    "start_time": "2022-10-14T16:53:49.200Z"
   },
   {
    "duration": 27,
    "start_time": "2022-10-14T16:53:50.200Z"
   },
   {
    "duration": 7,
    "start_time": "2022-10-14T16:53:50.232Z"
   },
   {
    "duration": 11,
    "start_time": "2022-10-14T16:53:50.241Z"
   },
   {
    "duration": 176728,
    "start_time": "2022-10-14T16:53:50.256Z"
   },
   {
    "duration": 1,
    "start_time": "2022-10-14T16:56:46.992Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:46.994Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:46.995Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:46.997Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:46.999Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.000Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.003Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.006Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.008Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.010Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.014Z"
   },
   {
    "duration": 0,
    "start_time": "2022-10-14T16:56:47.016Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.195px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
